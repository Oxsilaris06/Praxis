<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Strategica | Chatbot Llama 3.2 (Wasm/WebGPU/ONNX)</title>
    <link rel="icon" href="https://oxsilaris06.github.io/Praxis/maskable_icon(2).png" type="image/png">
    <link href="https://fonts.googleapis.com/css2?family=Oswald:wght@400;500&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Saira+Stencil+One&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@24,400,0,0" />
    
    <style>
        /* --- Styles CSS (Conserv√©s et adapt√©s pour l'affichage du chat) --- */
        :root {
            --bg-body: #121212; --bg-container: #1e1e1e; --bg-interactive: #2a2a2a;
            --text-primary: #e0e0e0; --text-secondary: #95a5a6; --border-color: #444444;
            --accent-blue: #5b9bd5; --accent-hover: #4a7aa5;
            --danger-red: #c0392b;
            --success-green: #27ae60;
            --llama-color: #f7931e; /* Llama orange/gold */
        }
        body.light-mode {
            --bg-body: #f0f2f5; --bg-container: #ffffff; --bg-interactive: #f8f9fa;
            --text-primary: #212529; --text-secondary: #6c757d; --accent-blue: #0033a0;
            --border-color: #dee2e6;
            --llama-color: #cc6600;
        }
        * { box-sizing: border-box; margin: 0; padding: 0; }
        html { font-size: 16px; }
        body { font-family: 'Oswald', sans-serif; background-color: var(--bg-body); color: var(--text-primary); line-height: 1.6; padding: 10px; padding-bottom: 90px; }
        .container { width: 100%; max-width: 1200px; margin: auto; background: var(--bg-container); padding: 20px; border-radius: 8px; border: 1px solid var(--border-color); }
        h1 { font-size: 2.2em; margin-bottom: 10px; color: var(--llama-color); font-family: 'Saira Stencil One', sans-serif; text-align: center; }
        h2 { font-size: 1.6em; color: var(--llama-color); border-bottom: 2px solid var(--llama-color); padding-bottom: 10px; margin-top: 20px; }
        p.subtitle { text-align: center; color: var(--text-secondary); margin-bottom: 30px; }
        .section { border: 1px solid var(--border-color); border-radius: 5px; padding: 20px; margin-top: 25px; background-color: var(--bg-interactive); }
        
        /* Styles Mod√®le/Console */
        #model-loader { text-align: center; }
        #model-loader button { background-color: var(--llama-color); color: white; padding: 14px 20px; border: none; border-radius: 5px; cursor: pointer; font-family: 'Oswald', sans-serif; font-size: 1.2em; margin-bottom: 15px; }
        #model-loader button:hover:not(:disabled) { background-color: var(--accent-hover); }
        #model-loader button:disabled { background-color: var(--text-secondary); cursor: not-allowed; }
        #model-status { color: var(--text-secondary); font-style: italic; min-height: 20px; transition: color 0.3s; }
        .progress-bar-container { width: 100%; background-color: var(--bg-body); border-radius: 5px; overflow: hidden; height: 25px; margin-top: 10px; display: none; }
        .progress-bar { width: 0%; height: 100%; background-color: var(--llama-color); text-align: center; line-height: 25px; color: white; transition: width 0.3s ease; }
        .console-container { margin-top: 20px; border: 1px solid var(--border-color); border-radius: 5px; padding: 10px; }
        .console-container h4 { text-align: left; margin: 0 0 10px 0; color: var(--text-secondary); font-family: 'Oswald', sans-serif; }
        .console-output { background-color: var(--bg-body); height: 150px; overflow-y: auto; font-family: monospace; font-size: 0.9em; padding: 10px; border-radius: 4px; text-align: left; white-space: pre-wrap; }
        .console-output .log-error { color: var(--danger-red); }
        .console-output .log-success { color: var(--success-green); }
        .console-output .log-debug { color: var(--accent-blue); }

        /* Styles Chat */
        #chat-interface { max-width: 800px; margin: 20px auto; padding: 20px; background-color: var(--bg-container); border-radius: 8px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2); }
        #messages-display { 
            height: 400px; overflow-y: scroll; padding: 15px; 
            border: 1px solid var(--border-color); margin-bottom: 15px; 
            background-color: var(--bg-interactive); border-radius: 4px;
        }
        #messages-display p { margin: 10px 0; padding: 5px; border-radius: 4px; line-height: 1.4; }
        #messages-display code { font-size: 0.9em; padding: 2px 4px; border-radius: 3px; background-color: var(--bg-body); }
        .user-message { text-align: right; }
        .assistant-message { text-align: left; }
        #userInput {
            flex-grow: 1; padding: 10px; border: 1px solid var(--border-color); 
            border-radius: 4px 0 0 4px; background-color: var(--bg-body); 
            color: var(--text-primary); font-size: 1em;
        }
        #sendMessageBtn {
            padding: 10px 15px; background-color: var(--llama-color); 
            color: white; border: none; border-radius: 0 4px 4px 0; 
            cursor: pointer; font-size: 1em;
        }
        #sendMessageBtn:hover:not(:disabled) { background-color: var(--accent-hover); }

        /* Dock Menu */
        .dock-menu { position: fixed; bottom: 10px; left: 50%; transform: translateX(-50%); display: flex; gap: 10px; padding: 10px; z-index: 1000; background-color: rgba(30, 30, 30, 0.7); border: 1px solid var(--border-color); border-radius: 35px; backdrop-filter: blur(10px); }
        .dock-menu-item { display: flex; align-items: center; justify-content: center; background-color: var(--bg-container); color: var(--text-primary); border: 1px solid var(--border-color); border-radius: 50%; width: 50px; height: 50px; font-size: 28px; cursor: pointer; text-decoration: none; }
    </style>
</head>
<body class="dark-mode">

    <div class="dock-menu" id="dockMenu">
        <a href="#" class="dock-menu-item" title="Accueil" onclick="location.reload()"><span class="material-symbols-outlined">home</span></a>
        <div class="dock-menu-item" id="darkModeToggle" title="Changer le th√®me"><span class="material-symbols-outlined" id="darkModeIcon">nightlight</span></div>
    </div>

    <div class="container">
        <h1>Strategica</h1>
        <p class="subtitle">Mod√®le Llama 3.2 (1B) - Wasm/WebGPU</p>

        <div class="section" id="model-loader-section">
            <h2>1. Initialisation de l'IA</h2>
            <div id="model-loader">
                <p style="color: var(--text-secondary); margin-bottom: 20px; text-align: center;">Le mod√®le Llama est charg√© via le pipeline Hugging Face qui utilise le moteur **WebAssembly (Wasm)** et l'acc√©l√©ration **WebGPU** (si disponible) via **ONNX**.</p>
                <button id="load-model-btn">Charger le Mod√®le de Chat (Llama-3.2-1B-Instruct)</button>
                <div id="model-status">Statut : Inactif</div>
                <div class="progress-bar-container" id="progress-container"><div class="progress-bar" id="progress-bar">0%</div></div>
                <div class="console-container" id="model-console-container" style="display:none;">
                    <h4>Journal d'Installation et de Debug üõ†Ô∏è</h4>
                    <div class="console-output" id="model-console-output"></div>
                </div>
            </div>
        </div>

        <div class="section" id="analysis-interface" style="display:none;">
            </div>
        
        <div class="section" id="chat-interface" style="display:none;">
            <h2>2. Chat Conversationnel</h2>
            <div id="messages-display">
                <p style="color: var(--text-secondary);">
                    **Syst√®me :** Bienvenue ! Chargez le mod√®le pour commencer √† discuter.
                </p>
            </div>

            <div style="display: flex;">
                <input type="text" id="userInput" placeholder="√âcrivez votre message ici..." disabled>
                <button id="sendMessageBtn" disabled>
                    Envoyer
                </button>
            </div>
        </div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/marked@4.0.10/marked.min.js"></script>
    
    <script type="module">
        import { pipeline } from 'https://cdn.jsdelivr.net/npm/@huggingface/transformers@latest'; 

        pdfjsLib.GlobalWorkerOptions.workerSrc = `https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.worker.min.js`;

        const MODEL_NAME = 'onnx-community/Llama-3.2-1B-Instruct-q4f16';
        const TASK = 'text-generation';

        // Prompt syst√®me optimis√© pour Llama-3.2-Instruct
        const SYSTEM_PROMPT_CONTENT = "Tu es un assistant IA conversationnel l√©ger et tr√®s rapide, bas√© sur Llama 3.2. Ton but est de fournir des r√©ponses br√®ves, pr√©cises et engageantes. Ton ton est toujours informatif et l√©g√®rement formel. Limite tes r√©ponses √† 3 phrases maximum. Ne d√©vie jamais de la question pos√©e. Si une requ√™te est ambigu√´, demande poliment une clarification. Rappelle-toi que la coh√©rence de tes r√©ponses est critique.";

        // Mappage DOM
        const dom = {
            loadModelBtn: document.getElementById('load-model-btn'), modelStatus: document.getElementById('model-status'),
            progressContainer: document.getElementById('progress-container'), progressBar: document.getElementById('progress-bar'),
            modelConsoleContainer: document.getElementById('model-console-container'), modelConsoleOutput: document.getElementById('model-console-output'),
            modelLoaderSection: document.getElementById('model-loader-section'),
            
            analysisInterface: document.getElementById('analysis-interface'), 
            
            chatInterface: document.getElementById('chat-interface'),
            userInput: document.getElementById('userInput'),
            sendMessageBtn: document.getElementById('sendMessageBtn'),
            messagesDisplay: document.getElementById('messages-display'),

            darkModeToggle: document.getElementById('darkModeToggle'),
            darkModeIcon: document.getElementById('darkModeIcon'),
        };

        let state = { pipe: null, messageHistory: [] }; 

        function logTo(consoleElement, message, type = 'info') {
            const logLine = document.createElement('p');
            logLine.className = 'log-line';
            const time = new Date().toLocaleTimeString();
            logLine.textContent = `[${time}] ${message}`;
            if (type === 'error') logLine.classList.add('log-error');
            else if (type === 'success') logLine.classList.add('log-success');
            else if (type === 'debug') logLine.classList.add('log-debug');
            consoleElement.appendChild(logLine);
            consoleElement.scrollTop = consoleElement.scrollHeight;
        }
        
        function switchToChatView() {
            dom.modelLoaderSection.style.display = 'none';
            dom.analysisInterface.style.display = 'none'; 
            dom.chatInterface.style.display = 'block';
        }

        async function loadModel() {
            if (state.pipe) {
                switchToChatView();
                return;
            }

            dom.loadModelBtn.disabled = true;
            dom.progressContainer.style.display = 'block';
            dom.modelConsoleContainer.style.display = 'block';
            dom.modelConsoleOutput.innerHTML = '';
            
            // LOG DE DEBUG 1 : Tentative de d√©tection WebGPU
            if ('gpu' in navigator) {
                logTo(dom.modelConsoleOutput, '‚ú® WebGPU d√©tect√©. Tentative d\'acc√©l√©ration GPU.', 'debug');
            } else {
                logTo(dom.modelConsoleOutput, 'üêå WebGPU non d√©tect√©. Utilisation de WebAssembly (WASM).', 'debug');
            }

            try {
                logTo(dom.modelConsoleOutput, `‚ñ∂Ô∏è D√©marrage du t√©l√©chargement du mod√®le ${MODEL_NAME}...`);
                
                state.pipe = await pipeline(TASK, MODEL_NAME, {
                    device: navigator.gpu ? 'webgpu' : 'wasm', 
                    progress_callback: (p) => {
                        // Mise √† jour de la barre de progression et du statut pour CHAQUE √©tape
                        if (typeof p.progress === 'number') {
                            const percentage = p.progress.toFixed(1);
                            dom.progressBar.style.width = percentage + '%';
                            dom.progressBar.textContent = `${percentage}%`;
                        }
                        
                        let statusText = `[${p.status}] ${p.file || ''}`;
                        dom.modelStatus.textContent = `Statut : ${statusText}`;

                        // Log seulement les √©tats cl√©s (initiate, done, ready) et non la progression par %
                        if (p.status === 'initiate' || p.status === 'done' || p.status === 'ready') {
                            const progressLog = typeof p.progress === 'number' ? `(${p.progress.toFixed(1)}%)` : '';
                            logTo(dom.modelConsoleOutput, `${statusText} ${progressLog}`);

                            if (p.status === 'ready') {
                                dom.modelStatus.textContent = 'Le mod√®le est pr√™t ! Initialisation...';
                            }
                        }
                    }
                });
                
                logTo(dom.modelConsoleOutput, '‚úÖ Mod√®le initialis√© avec succ√®s.', 'success');
                
                // LOG DE DEBUG 2 : Affichage du backend r√©el utilis√© (avec correction du bug 'toLowerCase')
                const deviceString = state.pipe?.model?.device || 'Ind√©fini (v√©rification du pipeline)';
                
                const backend = deviceString.toLowerCase().includes('gpu') ? 'WebGPU üöÄ (Acc√©l√©ration GPU)' 
                                : deviceString.toLowerCase().includes('wasm') ? 'WASM/CPU ‚öôÔ∏è'
                                : 'Inconnu (probablement WASM/CPU)';

                logTo(dom.modelConsoleOutput, `Backend d'inf√©rence r√©el utilis√© : ${backend}`, 'debug');

                switchToChatView();
                
                dom.sendMessageBtn.disabled = false; 
                dom.userInput.disabled = false;
                dom.userInput.focus();

            } catch (error) {
                logTo(dom.modelConsoleOutput, '‚ùå ERREUR CRITIQUE LORS DU CHARGEMENT', 'error');
                logTo(dom.modelConsoleOutput, `Message : ${error.message}`, 'error');
                dom.modelStatus.textContent = "Erreur de chargement. Consulter le journal.";
                dom.modelStatus.style.color = 'var(--danger-red)';
                dom.loadModelBtn.disabled = false;
            }
        }
        
        /**
         * Ajoute un message √† la fen√™tre de chat et g√®re le Markdown.
         */
        function addMessage(role, text) {
            const messageElement = document.createElement('p');
            const roleColor = role === 'user' ? 'var(--accent-blue)' : 
                              role === 'assistant' ? 'var(--llama-color)' : 'var(--text-secondary)';
            const roleLabel = role === 'user' ? '**Moi :**' : 
                              role === 'assistant' ? '**Llama 3.2 :**' : '**Syst√®me :**';
            
            const parsedText = marked.parse(text); 

            messageElement.innerHTML = `<span style="font-weight: bold; color: ${roleColor};">${roleLabel}</span> ${parsedText}`;
            messageElement.classList.add(`${role}-message`);
            
            dom.messagesDisplay.appendChild(messageElement);
            dom.messagesDisplay.scrollTop = dom.messagesDisplay.scrollHeight;
        }

        /**
         * Envoie le message de l'utilisateur au mod√®le et g√©n√®re la r√©ponse.
         */
        async function sendMessage() {
            const userText = dom.userInput.value.trim();
            if (!userText || !state.pipe) {
                return;
            }

            // LOG DE DEBUG 3 : V√©rification de la pertinence du prompt
            logTo(dom.modelConsoleOutput, `[User Prompt] : "${userText.substring(0, 50)}..."`, 'debug');

            addMessage('user', userText);
            dom.userInput.value = '';
            dom.sendMessageBtn.disabled = true;
            dom.userInput.disabled = true;
            
            state.messageHistory.push({ role: 'user', content: userText });

            // Construction de l'historique complet pour le pipeline (Systeme + Historique)
            const fullHistoryWithSystem = [
                { role: 'system', content: SYSTEM_PROMPT_CONTENT },
                ...state.messageHistory
            ];
            
            addMessage('system', 'Llama 3.2 r√©fl√©chit... ü§î');

            try {
                // LOG DE DEBUG 4 : Affichage de l'historique de conversation envoy√© au mod√®le
                logTo(dom.modelConsoleOutput, `[History Length] : ${fullHistoryWithSystem.length} messages envoy√©s.`, 'debug');

                const response = await state.pipe(fullHistoryWithSystem, {
                    max_new_tokens: 2048, 
                    temperature: 0.7,
                    top_p: 0.9,
                    do_sample: true,
                });

                const generatedHistory = response[0].generated_text;
                
                // On retire le message "r√©fl√©chit"
                if (dom.messagesDisplay.lastChild) dom.messagesDisplay.lastChild.remove(); 
                
                const assistantResponseObject = generatedHistory.slice(-1)[0];
                const assistantResponseText = assistantResponseObject.content;

                // LOG DE DEBUG 5 : V√©rification de la coh√©rence de la r√©ponse
                const responseLength = assistantResponseText.split(/[.!?]/).filter(s => s.trim().length > 0).length;
                logTo(dom.modelConsoleOutput, `[AI Response] : Coherence check: ${responseLength} phrases.`, responseLength > 3 ? 'error' : 'debug');

                addMessage('assistant', assistantResponseText);
                
                // Mise √† jour de l'historique interne (sans le message syst√®me)
                state.messageHistory = generatedHistory.filter(msg => msg.role !== 'system');
                
            } catch (error) {
                if (dom.messagesDisplay.lastChild) dom.messagesDisplay.lastChild.remove(); 
                addMessage('assistant', `D√©sol√©, une erreur est survenue lors de la g√©n√©ration. ${error.message}.`);
                logTo(dom.modelConsoleOutput, `‚ùå ERREUR DE G√âN√âRATION: ${error.message}`, 'error');
            } finally {
                dom.sendMessageBtn.disabled = false;
                dom.userInput.disabled = false;
                dom.userInput.focus();
            }
        }
        
        // **********************************************
        // √âCOUTEURS D'√âV√âNEMENTS
        // **********************************************

        dom.loadModelBtn.addEventListener('click', loadModel);
        dom.sendMessageBtn.addEventListener('click', sendMessage);
        
        dom.userInput.addEventListener('keypress', (e) => {
            if (e.key === 'Enter') {
                e.preventDefault(); 
                sendMessage();
            }
        });
        
        // Gestion du mode sombre/clair (conserv√©e)
        dom.darkModeToggle.addEventListener('click', () => {
            document.body.classList.toggle('light-mode'); document.body.classList.toggle('dark-mode');
            const isDarkMode = document.body.classList.contains('dark-mode');
            localStorage.setItem('theme', isDarkMode ? 'dark' : 'light');
            dom.darkModeIcon.textContent = isDarkMode ? 'nightlight' : 'clear_day';
        });

        document.addEventListener('DOMContentLoaded', () => {
            if (localStorage.getItem('theme') === 'light') {
                document.body.classList.replace('dark-mode', 'light-mode');
                dom.darkModeIcon.textContent = 'clear_day';
            }
        });
    </script>
</body>
</html>
